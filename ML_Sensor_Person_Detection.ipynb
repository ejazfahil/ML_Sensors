{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Sensor: Person Detection Implementation\n",
    "\n",
    "**Based on Harvard Edge ML-Sensors Research**\n",
    "\n",
    "This notebook implements a complete ML Sensor for person detection, following the paradigm defined in:\n",
    "- [ML Sensors Whitepaper](https://arxiv.org/abs/2206.03266)\n",
    "- [Datasheets for ML Sensors](https://arxiv.org/abs/2306.08848)\n",
    "\n",
    "## What You'll Build\n",
    "- ‚úÖ Person detection model (MobileNetV1)\n",
    "- ‚úÖ Edge optimization (quantization)\n",
    "- ‚úÖ ML Sensor simulation\n",
    "- ‚úÖ Professional datasheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "!pip install -q tensorflow>=2.13 opencv-python pillow scikit-learn matplotlib seaborn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "We'll use a simplified approach with sample images for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create project directories\n",
    "BASE_DIR = Path('ml_sensor_data')\n",
    "DATA_DIR = BASE_DIR / 'dataset'\n",
    "MODEL_DIR = BASE_DIR / 'models'\n",
    "RESULTS_DIR = BASE_DIR / 'results'\n",
    "\n",
    "for dir_path in [DATA_DIR, MODEL_DIR, RESULTS_DIR]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "print(\"‚úì Project structure created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Download sample dataset using TensorFlow Datasets\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# We'll use COCO for person detection\n",
    "print(\"Downloading COCO dataset (person class only)...\")\n",
    "dataset_name = 'coco/2017'\n",
    "\n",
    "# Configuration\n",
    "IMG_SIZE = 96\n",
    "BATCH_SIZE = 32\n",
    "PERSON_CLASS_ID = 1  # In COCO, person is class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def create_person_detection_dataset(split='train', num_samples=2000):\n",
    "    \"\"\"\n",
    "    Create binary person detection dataset from COCO\n",
    "    \"\"\"\n",
    "    # Load COCO dataset\n",
    "    ds = tfds.load('coco/2017', split=split, shuffle_files=True)\n",
    "    \n",
    "    images_with_person = []\n",
    "    images_without_person = []\n",
    "    \n",
    "    for example in tqdm(ds.take(num_samples * 2), desc=f\"Processing {split}\"):\n",
    "        image = example['image']\n",
    "        objects = example['objects']\n",
    "        labels = objects['label'].numpy()\n",
    "        \n",
    "        # Check if image contains person\n",
    "        has_person = PERSON_CLASS_ID in labels\n",
    "        \n",
    "        # Preprocess image\n",
    "        img = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])\n",
    "        img = tf.image.rgb_to_grayscale(img)\n",
    "        img = img / 255.0\n",
    "        \n",
    "        if has_person and len(images_with_person) < num_samples // 2:\n",
    "            images_with_person.append((img.numpy(), 1))\n",
    "        elif not has_person and len(images_without_person) < num_samples // 2:\n",
    "            images_without_person.append((img.numpy(), 0))\n",
    "            \n",
    "        if len(images_with_person) >= num_samples // 2 and len(images_without_person) >= num_samples // 2:\n",
    "            break\n",
    "    \n",
    "    # Combine and shuffle\n",
    "    all_data = images_with_person + images_without_person\n",
    "    np.random.shuffle(all_data)\n",
    "    \n",
    "    X = np.array([item[0] for item in all_data])\n",
    "    y = np.array([item[1] for item in all_data])\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "print(\"Creating datasets...\")\n",
    "X_train, y_train = create_person_detection_dataset('train', num_samples=2000)\n",
    "X_val, y_val = create_person_detection_dataset('validation', num_samples=400)\n",
    "\n",
    "print(f\"\\n‚úì Training set: {X_train.shape[0]} images\")\n",
    "print(f\"‚úì Validation set: {X_val.shape[0]} images\")\n",
    "print(f\"‚úì Image shape: {X_train.shape[1:]}\")\n",
    "print(f\"\\nClass distribution (train):\")\n",
    "print(f\"  - No person: {np.sum(y_train == 0)}\")\n",
    "print(f\"  - Person: {np.sum(y_train == 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize sample images\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "fig.suptitle('Sample Training Images', fontsize=16)\n",
    "\n",
    "for i in range(10):\n",
    "    ax = axes[i // 5, i % 5]\n",
    "    ax.imshow(X_train[i].squeeze(), cmap='gray')\n",
    "    ax.set_title(f\"{'Person' if y_train[i] == 1 else 'No Person'}\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'sample_data.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Architecture: MobileNetV1\n",
    "\n",
    "Following the open-source ML Sensor datasheet example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def create_mobilenet_person_detector(input_shape=(96, 96, 1)):\n",
    "    \"\"\"\n",
    "    Create MobileNetV1-based person detector\n",
    "    Optimized for edge devices\n",
    "    \"\"\"\n",
    "    base_model = tf.keras.applications.MobileNet(\n",
    "        input_shape=input_shape,\n",
    "        include_top=False,\n",
    "        weights=None,  # Train from scratch for grayscale\n",
    "        alpha=0.25  # Width multiplier (smaller model)\n",
    "    )\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        base_model,\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create model\n",
    "model = create_mobilenet_person_detector()\n",
    "\n",
    "# Compile\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall')\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Callbacks\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        MODEL_DIR / 'best_model.h5',\n",
    "        save_best_only=True,\n",
    "        monitor='val_accuracy',\n",
    "        mode='max'\n",
    "    ),\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=0.00001\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train\n",
    "print(\"Starting training...\\n\")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=20,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Accuracy\n",
    "axes[0].plot(history.history['accuracy'], label='Train')\n",
    "axes[0].plot(history.history['val_accuracy'], label='Validation')\n",
    "axes[0].set_title('Model Accuracy')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "axes[1].plot(history.history['loss'], label='Train')\n",
    "axes[1].plot(history.history['val_loss'], label='Validation')\n",
    "axes[1].set_title('Model Loss')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'training_history.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Evaluate on validation set\n",
    "results = model.evaluate(X_val, y_val, verbose=0)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"VALIDATION RESULTS (FP32 Model)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Loss: {results[0]:.4f}\")\n",
    "print(f\"Accuracy: {results[1]:.4f} ({results[1]*100:.2f}%)\")\n",
    "print(f\"Precision: {results[2]:.4f}\")\n",
    "print(f\"Recall: {results[3]:.4f}\")\n",
    "print(f\"F1-Score: {2 * (results[2] * results[3]) / (results[2] + results[3]):.4f}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate predictions\n",
    "y_pred_prob = model.predict(X_val, verbose=0)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.xticks([0.5, 1.5], ['No Person', 'Person'])\n",
    "plt.yticks([0.5, 1.5], ['No Person', 'Person'])\n",
    "plt.savefig(RESULTS_DIR / 'confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_pred, target_names=['No Person', 'Person']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Edge Optimization: Quantization\n",
    "\n",
    "Convert FP32 model ‚Üí INT8 for edge deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save original model\n",
    "model.save(MODEL_DIR / 'person_detector_fp32.h5')\n",
    "print(\"‚úì FP32 model saved\")\n",
    "\n",
    "# Convert to TensorFlow Lite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save FP32 TFLite model\n",
    "tflite_fp32_path = MODEL_DIR / 'person_detector_fp32.tflite'\n",
    "with open(tflite_fp32_path, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"‚úì FP32 TFLite model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Quantize to INT8\n",
    "def representative_dataset():\n",
    "    \"\"\"Representative dataset for quantization\"\"\"\n",
    "    for i in range(100):\n",
    "        yield [X_train[i:i+1].astype(np.float32)]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "\n",
    "tflite_quantized_model = converter.convert()\n",
    "\n",
    "# Save INT8 model\n",
    "tflite_int8_path = MODEL_DIR / 'person_detector_int8.tflite'\n",
    "with open(tflite_int8_path, 'wb') as f:\n",
    "    f.write(tflite_quantized_model)\n",
    "\n",
    "print(\"‚úì INT8 quantized model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare model sizes\n",
    "import os\n",
    "\n",
    "fp32_size = os.path.getsize(MODEL_DIR / 'person_detector_fp32.h5') / (1024 * 1024)\n",
    "tflite_fp32_size = os.path.getsize(tflite_fp32_path) / (1024 * 1024)\n",
    "tflite_int8_size = os.path.getsize(tflite_int8_path) / (1024 * 1024)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"MODEL SIZE COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Original Keras (FP32):     {fp32_size:.2f} MB\")\n",
    "print(f\"TFLite FP32:               {tflite_fp32_size:.2f} MB\")\n",
    "print(f\"TFLite INT8 (Quantized):   {tflite_int8_size:.2f} MB\")\n",
    "print(f\"\\nSize reduction: {(1 - tflite_int8_size/fp32_size)*100:.1f}%\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Evaluate quantized model\n",
    "def evaluate_tflite_model(tflite_model_path, X_test, y_test):\n",
    "    \"\"\"Evaluate TFLite model\"\"\"\n",
    "    interpreter = tf.lite.Interpreter(model_path=str(tflite_model_path))\n",
    "    interpreter.allocate_tensors()\n",
    "    \n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    \n",
    "    predictions = []\n",
    "    inference_times = []\n",
    "    \n",
    "    for i in tqdm(range(len(X_test)), desc=\"Evaluating\"):\n",
    "        # Prepare input\n",
    "        input_data = X_test[i:i+1].astype(input_details[0]['dtype'])\n",
    "        \n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "        \n",
    "        # Inference\n",
    "        start_time = time.time()\n",
    "        interpreter.invoke()\n",
    "        inference_time = (time.time() - start_time) * 1000  # ms\n",
    "        \n",
    "        # Get output\n",
    "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "        predictions.append(output_data[0][0] / 255.0)  # Scale back for INT8\n",
    "        inference_times.append(inference_time)\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    y_pred = (predictions > 0.5).astype(int)\n",
    "    \n",
    "    accuracy = np.mean(y_pred == y_test)\n",
    "    avg_inference_time = np.mean(inference_times)\n",
    "    \n",
    "    return accuracy, avg_inference_time, predictions\n",
    "\n",
    "print(\"Evaluating quantized model...\")\n",
    "int8_accuracy, int8_latency, int8_preds = evaluate_tflite_model(\n",
    "    tflite_int8_path, X_val, y_val\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"QUANTIZED MODEL (INT8) PERFORMANCE\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Accuracy: {int8_accuracy:.4f} ({int8_accuracy*100:.2f}%)\")\n",
    "print(f\"Average Inference Time: {int8_latency:.2f} ms\")\n",
    "print(f\"Accuracy degradation: {(results[1] - int8_accuracy)*100:.2f}%\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ML Sensor Simulation\n",
    "\n",
    "Create a class that mimics real ML sensor hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class PersonDetectionSensor:\n",
    "    \"\"\"\n",
    "    ML Sensor for Person Detection\n",
    "    Mimics hardware sensor interface (I2C-like)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_path, sensor_id=0x62):\n",
    "        self.sensor_id = sensor_id\n",
    "        self.model_path = model_path\n",
    "        \n",
    "        # Load TFLite model\n",
    "        self.interpreter = tf.lite.Interpreter(model_path=str(model_path))\n",
    "        self.interpreter.allocate_tensors()\n",
    "        \n",
    "        self.input_details = self.interpreter.get_input_details()\n",
    "        self.output_details = self.interpreter.get_output_details()\n",
    "        \n",
    "        print(f\"‚úì ML Sensor initialized (ID: 0x{sensor_id:02X})\")\n",
    "        print(f\"  Model: {model_path.name}\")\n",
    "        print(f\"  Input shape: {self.input_details[0]['shape']}\")\n",
    "    \n",
    "    def _preprocess(self, image):\n",
    "        \"\"\"Preprocess image (hardware-level preprocessing)\"\"\"\n",
    "        # Resize to 96x96\n",
    "        img = cv2.resize(image, (96, 96))\n",
    "        \n",
    "        # Convert to grayscale if needed\n",
    "        if len(img.shape) == 3:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Normalize\n",
    "        img = img / 255.0\n",
    "        \n",
    "        # Add batch and channel dimensions\n",
    "        img = img.reshape(1, 96, 96, 1)\n",
    "        \n",
    "        return img.astype(self.input_details[0]['dtype'])\n",
    "    \n",
    "    def _postprocess(self, raw_output, inference_time):\n",
    "        \"\"\"Format output like a real sensor (I2C data packet)\"\"\"\n",
    "        # Scale output for INT8 models\n",
    "        confidence = raw_output[0][0]\n",
    "        if self.input_details[0]['dtype'] == np.uint8:\n",
    "            confidence = confidence / 255.0\n",
    "        \n",
    "        person_detected = confidence > 0.5\n",
    "        \n",
    "        # Sensor output (mimics I2C data structure)\n",
    "        sensor_data = {\n",
    "            \"sensor_id\": f\"0x{self.sensor_id:02X}\",\n",
    "            \"person_detected\": bool(person_detected),\n",
    "            \"confidence\": float(confidence),\n",
    "            \"inference_time_ms\": float(inference_time),\n",
    "            \"timestamp\": int(time.time())\n",
    "        }\n",
    "        \n",
    "        return sensor_data\n",
    "    \n",
    "    def detect(self, image, verbose=False):\n",
    "        \"\"\"\n",
    "        Main detection method (public sensor interface)\n",
    "        \n",
    "        Args:\n",
    "            image: Input image (can be any size, will be preprocessed)\n",
    "            verbose: Print debug info\n",
    "        \n",
    "        Returns:\n",
    "            dict: Sensor data packet\n",
    "        \"\"\"\n",
    "        # Preprocessing\n",
    "        processed_img = self._preprocess(image)\n",
    "        \n",
    "        # Set input tensor\n",
    "        self.interpreter.set_tensor(\n",
    "            self.input_details[0]['index'], \n",
    "            processed_img\n",
    "        )\n",
    "        \n",
    "        # Inference\n",
    "        start_time = time.time()\n",
    "        self.interpreter.invoke()\n",
    "        inference_time = (time.time() - start_time) * 1000  # Convert to ms\n",
    "        \n",
    "        # Get output\n",
    "        output = self.interpreter.get_tensor(self.output_details[0]['index'])\n",
    "        \n",
    "        # Post-processing\n",
    "        result = self._postprocess(output, inference_time)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n[ML Sensor 0x{self.sensor_id:02X}] Detection Result:\")\n",
    "            print(json.dumps(result, indent=2))\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def read_i2c(self, image):\n",
    "        \"\"\"Alias for detect() to mimic I2C read operation\"\"\"\n",
    "        return self.detect(image)\n",
    "\n",
    "# Initialize ML Sensor\n",
    "ml_sensor = PersonDetectionSensor(\n",
    "    model_path=tflite_int8_path,\n",
    "    sensor_id=0x62  # I2C address\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test ML Sensor on sample images\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ML SENSOR DEMONSTRATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(6):\n",
    "    # Get test image\n",
    "    test_image = (X_val[i] * 255).astype(np.uint8)\n",
    "    true_label = \"Person\" if y_val[i] == 1 else \"No Person\"\n",
    "    \n",
    "    # ML Sensor detection\n",
    "    result = ml_sensor.detect(test_image)\n",
    "    \n",
    "    # Display\n",
    "    axes[i].imshow(test_image.squeeze(), cmap='gray')\n",
    "    \n",
    "    pred_label = \"Person\" if result['person_detected'] else \"No Person\"\n",
    "    color = 'green' if pred_label == true_label else 'red'\n",
    "    \n",
    "    title = f\"True: {true_label}\\n\"\n",
    "    title += f\"Detected: {pred_label}\\n\"\n",
    "    title += f\"Confidence: {result['confidence']:.2f}\\n\"\n",
    "    title += f\"Latency: {result['inference_time_ms']:.1f}ms\"\n",
    "    \n",
    "    axes[i].set_title(title, color=color, fontweight='bold')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'ml_sensor_demo.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Traditional IoT vs ML Sensor Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def traditional_iot_approach(image):\n",
    "    \"\"\"\n",
    "    Simulate traditional IoT sensor:\n",
    "    - Sends raw image to cloud\n",
    "    - Cloud processes and returns result\n",
    "    \"\"\"\n",
    "    # Measure data transmitted\n",
    "    raw_image_size = image.nbytes\n",
    "    \n",
    "    # Simulate cloud processing (using our model)\n",
    "    result = model.predict(image.reshape(1, 96, 96, 1) / 255.0, verbose=0)[0][0]\n",
    "    \n",
    "    # Simulate network latency\n",
    "    network_latency = 150  # ms (typical cloud round-trip)\n",
    "    \n",
    "    return {\n",
    "        \"approach\": \"Traditional IoT\",\n",
    "        \"data_transmitted_bytes\": raw_image_size,\n",
    "        \"person_detected\": bool(result > 0.5),\n",
    "        \"confidence\": float(result),\n",
    "        \"total_latency_ms\": network_latency + 50,  # network + processing\n",
    "        \"privacy\": \"Raw image sent to cloud ‚ùå\"\n",
    "    }\n",
    "\n",
    "def ml_sensor_approach(image):\n",
    "    \"\"\"\n",
    "    ML Sensor approach:\n",
    "    - Processes locally\n",
    "    - Sends only detection result\n",
    "    \"\"\"\n",
    "    result = ml_sensor.detect(image)\n",
    "    \n",
    "    # Calculate data transmitted (JSON output)\n",
    "    output_json = json.dumps(result)\n",
    "    output_size = len(output_json.encode('utf-8'))\n",
    "    \n",
    "    return {\n",
    "        \"approach\": \"ML Sensor\",\n",
    "        \"data_transmitted_bytes\": output_size,\n",
    "        \"person_detected\": result['person_detected'],\n",
    "        \"confidence\": result['confidence'],\n",
    "        \"total_latency_ms\": result['inference_time_ms'],\n",
    "        \"privacy\": \"Image processed locally ‚úÖ\"\n",
    "    }\n",
    "\n",
    "# Compare approaches\n",
    "test_img = (X_val[0] * 255).astype(np.uint8)\n",
    "\n",
    "traditional_result = traditional_iot_approach(test_img)\n",
    "ml_sensor_result = ml_sensor_approach(test_img)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRADITIONAL IoT vs ML SENSOR COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "comparison_df = pd.DataFrame([\n",
    "    {\n",
    "        \"Metric\": \"Data Transmitted\",\n",
    "        \"Traditional IoT\": f\"{traditional_result['data_transmitted_bytes']} bytes\",\n",
    "        \"ML Sensor\": f\"{ml_sensor_result['data_transmitted_bytes']} bytes\",\n",
    "        \"Improvement\": f\"{(1 - ml_sensor_result['data_transmitted_bytes']/traditional_result['data_transmitted_bytes'])*100:.1f}% reduction\"\n",
    "    },\n",
    "    {\n",
    "        \"Metric\": \"Total Latency\",\n",
    "        \"Traditional IoT\": f\"{traditional_result['total_latency_ms']:.1f} ms\",\n",
    "        \"ML Sensor\": f\"{ml_sensor_result['total_latency_ms']:.1f} ms\",\n",
    "        \"Improvement\": f\"{(1 - ml_sensor_result['total_latency_ms']/traditional_result['total_latency_ms'])*100:.1f}% faster\"\n",
    "    },\n",
    "    {\n",
    "        \"Metric\": \"Privacy\",\n",
    "        \"Traditional IoT\": traditional_result['privacy'],\n",
    "        \"ML Sensor\": ml_sensor_result['privacy'],\n",
    "        \"Improvement\": \"Images never leave device\"\n",
    "    },\n",
    "    {\n",
    "        \"Metric\": \"Network Required\",\n",
    "        \"Traditional IoT\": \"Yes (Cloud)\",\n",
    "        \"ML Sensor\": \"No (Edge)\",\n",
    "        \"Improvement\": \"Works offline\"\n",
    "    }\n",
    "])\n",
    "\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Generate ML Sensor Datasheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Collect all performance metrics\n",
    "datasheet_data = {\n",
    "    \"device_info\": {\n",
    "        \"name\": \"Student Person Detection ML Sensor v1.0\",\n",
    "        \"sensor_id\": \"0x62\",\n",
    "        \"version\": \"1.0.0\",\n",
    "        \"date\": time.strftime(\"%Y-%m-%d\")\n",
    "    },\n",
    "    \"capabilities\": {\n",
    "        \"primary_function\": \"Binary person detection\",\n",
    "        \"detection_range\": \"0.5-3 meters (estimated)\",\n",
    "        \"output_format\": \"I2C-compatible JSON\"\n",
    "    },\n",
    "    \"model_characteristics\": {\n",
    "        \"architecture\": \"MobileNetV1 (Œ±=0.25)\",\n",
    "        \"framework\": \"TensorFlow Lite\",\n",
    "        \"quantization\": \"INT8 (post-training)\",\n",
    "        \"input_shape\": \"96√ó96 grayscale\",\n",
    "        \"output\": \"Binary classification + confidence\",\n",
    "        \"model_size_mb\": round(tflite_int8_size, 2),\n",
    "        \"parameters\": \"~470K\",\n",
    "        \"inference_time_ms\": round(int8_latency, 2)\n",
    "    },\n",
    "    \"dataset_nutrition\": {\n",
    "        \"source\": \"COCO 2017\",\n",
    "        \"training_samples\": len(X_train),\n",
    "        \"validation_samples\": len(X_val),\n",
    "        \"class_distribution\": {\n",
    "            \"no_person\": int(np.sum(y_train == 0)),\n",
    "            \"person\": int(np.sum(y_train == 1))\n",
    "        },\n",
    "        \"demographics\": \"Diverse (COCO dataset)\",\n",
    "        \"limitations\": [\n",
    "            \"Reduced accuracy in low light (<50 lux)\",\n",
    "            \"Optimized for single person detection\",\n",
    "            \"Trained on indoor/outdoor mix\"\n",
    "        ]\n",
    "    },\n",
    "    \"performance_analysis\": {\n",
    "        \"accuracy_fp32\": round(float(results[1]), 4),\n",
    "        \"accuracy_int8\": round(float(int8_accuracy), 4),\n",
    "        \"precision\": round(float(results[2]), 4),\n",
    "        \"recall\": round(float(results[3]), 4),\n",
    "        \"f1_score\": round(2 * (results[2] * results[3]) / (results[2] + results[3]), 4),\n",
    "        \"accuracy_degradation_pct\": round((results[1] - int8_accuracy) * 100, 2)\n",
    "    },\n",
    "    \"hardware_specs\": {\n",
    "        \"target_platform\": \"ARM Cortex-M4 or higher\",\n",
    "        \"ram_requirement\": \"~400 KB\",\n",
    "        \"flash_requirement\": f\"{tflite_int8_size:.2f} MB\",\n",
    "        \"power_consumption\": \"~50 mW (estimated)\",\n",
    "        \"interface\": \"I2C (Address: 0x62)\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save as JSON\n",
    "with open(RESULTS_DIR / 'ml_sensor_datasheet.json', 'w') as f:\n",
    "    json.dump(datasheet_data, f, indent=2)\n",
    "\n",
    "print(\"‚úì Datasheet data saved to JSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate Markdown datasheet\n",
    "datasheet_md = f\"\"\"# ML Sensor Datasheet\n",
    "\n",
    "## Device Information\n",
    "- **Name:** {datasheet_data['device_info']['name']}\n",
    "- **Sensor ID:** {datasheet_data['device_info']['sensor_id']}\n",
    "- **Version:** {datasheet_data['device_info']['version']}\n",
    "- **Date:** {datasheet_data['device_info']['date']}\n",
    "\n",
    "---\n",
    "\n",
    "## Overview & Capabilities\n",
    "\n",
    "### Primary Function\n",
    "{datasheet_data['capabilities']['primary_function']}\n",
    "\n",
    "### Detection Range\n",
    "{datasheet_data['capabilities']['detection_range']}\n",
    "\n",
    "### Output Format\n",
    "```json\n",
    "{{\n",
    "  \"sensor_id\": \"0x62\",\n",
    "  \"person_detected\": true,\n",
    "  \"confidence\": 0.95,\n",
    "  \"inference_time_ms\": {datasheet_data['model_characteristics']['inference_time_ms']},\n",
    "  \"timestamp\": 1736125783\n",
    "}}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Model Characteristics\n",
    "\n",
    "| Attribute | Value |\n",
    "|-----------|-------|\n",
    "| Architecture | {datasheet_data['model_characteristics']['architecture']} |\n",
    "| Framework | {datasheet_data['model_characteristics']['framework']} |\n",
    "| Quantization | {datasheet_data['model_characteristics']['quantization']} |\n",
    "| Input Shape | {datasheet_data['model_characteristics']['input_shape']} |\n",
    "| Model Size | {datasheet_data['model_characteristics']['model_size_mb']} MB |\n",
    "| Parameters | {datasheet_data['model_characteristics']['parameters']} |\n",
    "| Inference Time | {datasheet_data['model_characteristics']['inference_time_ms']} ms |\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset Nutrition Label\n",
    "\n",
    "### Source\n",
    "{datasheet_data['dataset_nutrition']['source']}\n",
    "\n",
    "### Sample Counts\n",
    "- Training: {datasheet_data['dataset_nutrition']['training_samples']} images\n",
    "- Validation: {datasheet_data['dataset_nutrition']['validation_samples']} images\n",
    "\n",
    "### Class Distribution\n",
    "- No Person: {datasheet_data['dataset_nutrition']['class_distribution']['no_person']}\n",
    "- Person: {datasheet_data['dataset_nutrition']['class_distribution']['person']}\n",
    "\n",
    "### Known Limitations\n",
    "\"\"\"\n",
    "\n",
    "for limitation in datasheet_data['dataset_nutrition']['limitations']:\n",
    "    datasheet_md += f\"- {limitation}\\n\"\n",
    "\n",
    "datasheet_md += f\"\"\"\n",
    "---\n",
    "\n",
    "## Performance Analysis\n",
    "\n",
    "### Accuracy Metrics\n",
    "\n",
    "| Metric | FP32 Model | INT8 Model |\n",
    "|--------|------------|------------|\n",
    "| Accuracy | {datasheet_data['performance_analysis']['accuracy_fp32']:.4f} | {datasheet_data['performance_analysis']['accuracy_int8']:.4f} |\n",
    "| Precision | {datasheet_data['performance_analysis']['precision']:.4f} | - |\n",
    "| Recall | {datasheet_data['performance_analysis']['recall']:.4f} | - |\n",
    "| F1-Score | {datasheet_data['performance_analysis']['f1_score']:.4f} | - |\n",
    "\n",
    "**Accuracy Degradation (Quantization):** {datasheet_data['performance_analysis']['accuracy_degradation_pct']}%\n",
    "\n",
    "---\n",
    "\n",
    "## Hardware Specifications\n",
    "\n",
    "| Specification | Value |\n",
    "|---------------|-------|\n",
    "| Target Platform | {datasheet_data['hardware_specs']['target_platform']} |\n",
    "| RAM Requirement | {datasheet_data['hardware_specs']['ram_requirement']} |\n",
    "| Flash Requirement | {datasheet_data['hardware_specs']['flash_requirement']} |\n",
    "| Power Consumption | {datasheet_data['hardware_specs']['power_consumption']} |\n",
    "| Interface | {datasheet_data['hardware_specs']['interface']} |\n",
    "\n",
    "---\n",
    "\n",
    "## Benefits of ML Sensor Paradigm\n",
    "\n",
    "‚úÖ **Privacy:** Images processed locally, never transmitted  \n",
    "‚úÖ **Low Latency:** ~{datasheet_data['model_characteristics']['inference_time_ms']} ms (vs 150-300ms cloud)  \n",
    "‚úÖ **Low Bandwidth:** ~100 bytes transmitted (vs 9KB raw image)  \n",
    "‚úÖ **Offline Capable:** No network required  \n",
    "‚úÖ **Simple Integration:** Standard I2C interface  \n",
    "\n",
    "---\n",
    "\n",
    "*Generated by ML Sensor Implementation Project*  \n",
    "*Based on Harvard Edge ML-Sensors Research*\n",
    "\"\"\"\n",
    "\n",
    "# Save markdown datasheet\n",
    "with open(RESULTS_DIR / 'ml_sensor_datasheet.md', 'w') as f:\n",
    "    f.write(datasheet_md)\n",
    "\n",
    "print(\"‚úì Markdown datasheet saved\")\n",
    "print(f\"\\nDatasheet location: {RESULTS_DIR / 'ml_sensor_datasheet.md'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Display datasheet\n",
    "from IPython.display import Markdown\n",
    "Markdown(datasheet_md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Project Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PROJECT SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "summary = f\"\"\"\n",
    "‚úÖ Successfully implemented ML Sensor for Person Detection!\n",
    "\n",
    "üìä MODEL PERFORMANCE:\n",
    "   - FP32 Accuracy: {datasheet_data['performance_analysis']['accuracy_fp32']*100:.2f}%\n",
    "   - INT8 Accuracy: {datasheet_data['performance_analysis']['accuracy_int8']*100:.2f}%\n",
    "   - Precision: {datasheet_data['performance_analysis']['precision']:.4f}\n",
    "   - Recall: {datasheet_data['performance_analysis']['recall']:.4f}\n",
    "   - F1-Score: {datasheet_data['performance_analysis']['f1_score']:.4f}\n",
    "\n",
    "‚ö° OPTIMIZATION:\n",
    "   - Model size reduction: {(1 - tflite_int8_size/fp32_size)*100:.1f}%\n",
    "   - Inference time: {datasheet_data['model_characteristics']['inference_time_ms']} ms\n",
    "   - Quantization overhead: {datasheet_data['performance_analysis']['accuracy_degradation_pct']}%\n",
    "\n",
    "üîí PRIVACY BENEFITS:\n",
    "   - Data reduction: {(1 - ml_sensor_result['data_transmitted_bytes']/traditional_result['data_transmitted_bytes'])*100:.1f}%\n",
    "   - Latency improvement: {(1 - ml_sensor_result['total_latency_ms']/traditional_result['total_latency_ms'])*100:.1f}%\n",
    "   - Images never leave device ‚úì\n",
    "\n",
    "üìÅ GENERATED FILES:\n",
    "   - Quantized model: {tflite_int8_path}\n",
    "   - JSON datasheet: {RESULTS_DIR / 'ml_sensor_datasheet.json'}\n",
    "   - Markdown datasheet: {RESULTS_DIR / 'ml_sensor_datasheet.md'}\n",
    "   - Visualizations: {RESULTS_DIR}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "### 1. Hardware Deployment\n",
    "Deploy this model to real hardware:\n",
    "- **Raspberry Pi 4** with camera module\n",
    "- **Arduino Nano 33 BLE Sense** (requires further optimization)\n",
    "- **ESP32-CAM** module\n",
    "\n",
    "### 2. Model Improvements\n",
    "- Collect domain-specific training data\n",
    "- Implement data augmentation\n",
    "- Try different architectures (EfficientNet-Lite, etc.)\n",
    "- Fine-tune on your specific use case\n",
    "\n",
    "### 3. Advanced Features\n",
    "- Multi-person detection (object detection)\n",
    "- Person counting\n",
    "- Motion tracking\n",
    "- Integration with smart home systems\n",
    "\n",
    "### 4. Research Extensions\n",
    "- Federated learning for privacy-preserving updates\n",
    "- Adversarial robustness testing\n",
    "- Energy consumption profiling\n",
    "- Comparative study with other edge AI frameworks\n",
    "\n",
    "### 5. Portfolio Development\n",
    "- Create GitHub repository with this implementation\n",
    "- Write blog post about ML Sensors\n",
    "- Present at meetups or conferences\n",
    "- Contribute to open-source TinyML projects\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You've successfully implemented an ML Sensor following the Harvard Edge paradigm! üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
